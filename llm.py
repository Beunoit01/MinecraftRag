import os
import json
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import torch
# Importation n√©cessaire pour le LLM local
from llama_cpp import Llama

# --- Configuration ---
# Chemin vers le dossier de la base ChromaDB persistante
PERSIST_DIRECTORY = "chroma_db_climate_facts"
# Nom de la collection dans ChromaDB
COLLECTION_NAME = "climate_facts_chunks"
# Nom du mod√®le d'embedding (DOIT √™tre le m√™me que celui utilis√© pour cr√©er les embeddings)
MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'
# Nombre de chunks pertinents √† r√©cup√©rer de ChromaDB
NUM_RESULTS_TO_RETRIEVE = 5 # Plus de contexte pour l'analyse des fake news

# --- Configuration du LLM Local (llama-cpp-python) ---
# *** MODIFIEZ CECI: Chemin vers votre fichier mod√®le GGUF t√©l√©charg√© ***
MODEL_PATH = "/home/benoitv/Documents/Software/Phi-3-mini-4k-instruct-q4.gguf" # EXEMPLE: "/home/user/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf"

# Param√®tres pour llama-cpp-python
N_GPU_LAYERS = 30   # Nombre de couches √† d√©charger sur le GPU.
                    # √Ä AJUSTER pour votre RTX 3060 Ti 8GB !
                    # Commencez vers 30-35 pour un mod√®le 7B Q4/Q5.
                    # Si vous avez des erreurs "CUDA out of memory", r√©duisez cette valeur.
                    # Si c'est trop lent, essayez d'augmenter (si la VRAM le permet).
                    # Mettre 0 pour utiliser uniquement le CPU (tr√®s lent).
N_CTX = 2048        # Taille maximale du contexte (tokens) que le mod√®le peut g√©rer.
                    # V√©rifiez la taille support√©e par le mod√®le GGUF choisi. 2048 est souvent s√ªr.
N_BATCH = 512       # Taille du batch pour le traitement du prompt (peut influencer la VRAM).

# --- Initialisation du Mod√®le d'Embedding (inchang√©) ---
print(f"\nChargement du mod√®le d'embedding local: '{MODEL_NAME}'")
try:
    device_embed = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Utilisation du p√©riph√©rique pour l'embedding: {device_embed.upper()}")
    embedding_model = SentenceTransformer(MODEL_NAME, device=device_embed)
    print("Mod√®le d'embedding charg√©.")
except Exception as e:
    print(f"Erreur critique lors du chargement du mod√®le d'embedding: {e}")
    exit()

# --- Initialisation de ChromaDB (inchang√©) ---
print(f"\nConnexion √† la base de donn√©es ChromaDB persistante: '{PERSIST_DIRECTORY}'")
try:
    client = chromadb.PersistentClient(path=PERSIST_DIRECTORY)
    default_ef = embedding_functions.DefaultEmbeddingFunction() # Juste pour obtenir la collection
    collection = client.get_collection(
        name=COLLECTION_NAME,
        embedding_function=default_ef
    )
    print(f"Connect√© √† la collection '{COLLECTION_NAME}' contenant {collection.count()} √©l√©ments.")
except Exception as e:
    print(f"Erreur critique lors de la connexion √† ChromaDB ou √† la collection: {e}")
    exit()

# --- Initialisation du LLM Local ---
print(f"\nChargement du LLM local depuis: '{MODEL_PATH}'")
llm_model = None # Initialiser √† None
if not os.path.exists(MODEL_PATH):
    print(f"Erreur: Le fichier mod√®le GGUF '{MODEL_PATH}' n'a pas √©t√© trouv√©.")
    print("Veuillez v√©rifier le chemin dans la configuration MODEL_PATH.")
else:
    try:
        llm_model = Llama(
            model_path=MODEL_PATH,
            n_gpu_layers=N_GPU_LAYERS,
            n_ctx=N_CTX,
            n_batch=N_BATCH,
            verbose=True # Mettre √† False pour moins de messages de llama.cpp
        )
        print("Mod√®le LLM local charg√© avec succ√®s.")
    except Exception as e:
        print(f"Erreur lors du chargement du mod√®le LLM local: {e}")
        print("V√©rifiez le chemin du mod√®le, les param√®tres (n_gpu_layers) et l'installation de llama-cpp-python avec support GPU.")
        # Le script continuera sans LLM si le chargement √©choue

def analyze_climate_claim(claim: str):
    """
    Analyse une affirmation sur le climat pour d√©tecter les fake news.
    Retourne une √©valuation bas√©e sur les sources scientifiques fiables.
    """
    print(f"\nAnalyse de l'affirmation: '{claim}'")

    # 1. Cr√©er l'embedding de l'affirmation
    print("  -> Cr√©ation de l'embedding pour l'affirmation...")
    try:
        claim_embedding = embedding_model.encode(claim, device=device_embed).tolist()
        print("  -> Embedding de l'affirmation cr√©√©.")
    except Exception as e:
        print(f"  -> Erreur lors de la cr√©ation de l'embedding: {e}")
        return "Impossible d'analyser cette affirmation (erreur technique)."

    # 2. Rechercher dans les sources scientifiques fiables
    print(f"  -> Recherche dans les sources scientifiques fiables...")
    try:
        results = collection.query(
            query_embeddings=[claim_embedding],
            n_results=NUM_RESULTS_TO_RETRIEVE,
            include=['documents', 'metadatas', 'distances']
        )
        print(f"  -> Trouv√© {len(results.get('ids', [[]])[0])} source(s) pertinente(s).")
    except Exception as e:
        print(f"  -> Erreur lors de la recherche: {e}")
        return "Impossible d'acc√©der aux sources scientifiques (erreur base de donn√©es)."

    context_chunks = results.get('documents', [[]])[0]
    if not context_chunks:
        return "‚ùì **INFORMATION INSUFFISANTE** - Aucune source scientifique fiable trouv√©e pour √©valuer cette affirmation."

    sources = [meta.get('source', 'Inconnue') for meta in results.get('metadatas', [[]])[0]]
    distances = results.get('distances', [[]])[0]
    
    # 3. Construire le prompt sp√©cialis√© pour la d√©tection de fake news
    context_string = "\n\n---\n\n".join(context_chunks)
    
    prompt = f"""You are a climate science fact-checker. Based ONLY on the provided scientific sources (IPCC reports, Environmental Defense Fund), analyze the following claim:

CLAIM TO ANALYZE: "{claim}"

SCIENTIFIC SOURCES:
---
{context_string}
---

Provide a fact-check analysis in the following format:
1. VERDICT: [TRUE/FALSE/PARTIALLY TRUE/INSUFFICIENT DATA]
2. CONFIDENCE: [HIGH/MEDIUM/LOW]
3. EXPLANATION: Brief explanation based on the scientific sources
4. SOURCES: Which sources support your verdict

Be precise and cite specific scientific evidence. If the claim contradicts established science, clearly explain why.
"""

    print("\n  -> Analyse en cours par l'IA...")

    # 4. G√©n√©rer l'analyse avec le LLM
    if llm_model:
        try:
            output = llm_model(
                prompt,
                max_tokens=500,
                stop=["CLAIM TO ANALYZE:", "\n\n---", "SCIENTIFIC SOURCES:"],
                echo=False,
                temperature=0.2  # Plus d√©terministe pour les fact-checks
            )
            analysis = output['choices'][0]['text'].strip()
            
            # Ajouter les m√©tadonn√©es des sources
            sources_info = "\n\n**Sources consult√©es:**\n"
            for src, dist in zip(sources, distances):
                sources_info += f"- {src} (pertinence: {(1-dist)*100:.1f}%)\n"
            
            return analysis + sources_info

        except Exception as e:
            print(f"  -> Erreur lors de l'analyse: {e}")
            return "Erreur lors de l'analyse par l'IA."
    else:
        # Version de secours sans LLM
        return f"üîç **SOURCES TROUV√âES** (LLM non disponible):\n\n{context_string}\n\n**Sources:** {', '.join(set(sources))}"


# --- Fonction pour g√©rer une requ√™te RAG (adapt√©e pour LLM local) ---

def answer_query_rag(query: str):
    """
    Prend une question, trouve le contexte pertinent dans ChromaDB,
    et g√©n√®re une r√©ponse en utilisant le LLM local configur√©.
    """
    print(f"\nTraitement de la question: '{query}'")

    # 1. Cr√©er l'embedding de la question (inchang√©)
    print("  -> Cr√©ation de l'embedding pour la question...")
    try:
        # Utiliser le m√™me device que pour le chargement du mod√®le d'embedding
        query_embedding = embedding_model.encode(query, device=device_embed).tolist()
        print("  -> Embedding de la question cr√©√©.")
    except Exception as e:
        print(f"  -> Erreur lors de la cr√©ation de l'embedding de la question: {e}")
        return "D√©sol√©, je n'ai pas pu traiter votre question (erreur d'embedding)."

    # 2. Interroger ChromaDB pour trouver les chunks pertinents (inchang√©)
    print(f"  -> Recherche des {NUM_RESULTS_TO_RETRIEVE} chunks pertinents dans ChromaDB...")
    try:
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=NUM_RESULTS_TO_RETRIEVE,
            include=['documents', 'metadatas', 'distances']
        )
        print(f"  -> Recherche termin√©e. Trouv√© {len(results.get('ids', [[]])[0])} r√©sultat(s).")
    except Exception as e:
        print(f"  -> Erreur lors de la recherche dans ChromaDB: {e}")
        return "D√©sol√©, je n'ai pas pu rechercher les informations pertinentes (erreur base de donn√©es)."

    context_chunks = results.get('documents', [[]])[0]
    if not context_chunks:
        print("  -> Aucun chunk pertinent trouv√© dans la base de donn√©es.")
        return "D√©sol√©, je n'ai trouv√© aucune information pertinente dans ma base de connaissances pour r√©pondre √† votre question."

    sources = [meta.get('source', 'Inconnue') for meta in results.get('metadatas', [[]])[0]]
    distances = results.get('distances', [[]])[0]
    print("  -> Sources r√©cup√©r√©es (distances):")
    for src, dist in zip(sources, distances):
        print(f"    - {src} (distance: {dist:.4f})")

    # 3. Construire le prompt pour le LLM (potentiellement √† adapter au format du mod√®le)
    context_string = "\n\n---\n\n".join(context_chunks)

    # *** MODIFICATION: Prompt en Anglais ***
    # Simple English prompt format often suitable for Instruct GGUF models.
    # Some models might prefer specific formats (Alpaca, ChatML...).
    # Check the model's page on Hugging Face if needed.
    prompt = f"""Context:
---
{context_string}
---
Question: {query}

Answer based solely on the provided context:
"""
    # *** Fin de la modification ***

    print("\n  -> Prompt construit pour le LLM local (d√©but):")
    print(prompt[:500] + "...")

    # 4. G√©n√©rer la r√©ponse avec le LLM local (si charg√©)
    if llm_model:
        print("\n  -> Envoi de la requ√™te au LLM local...")
        try:
            # Appel √† llama-cpp-python pour la g√©n√©ration
            output = llm_model(
                prompt,
                max_tokens=300,  # Limite le nombre de tokens g√©n√©r√©s pour la r√©ponse
                stop=["Question:", "\n", "Context:"], # Arr√™te la g√©n√©ration si le mod√®le commence √† poser une autre question, saute une ligne ou r√©p√®te le contexte
                echo=False      # Ne r√©p√®te pas le prompt dans la sortie
            )
            # Extraire le texte de la r√©ponse
            final_answer = output['choices'][0]['text'].strip()
            print("  -> R√©ponse re√ßue du LLM local.")
            return final_answer

        except Exception as e:
            print(f"  -> Erreur lors de la g√©n√©ration de la r√©ponse par le LLM local: {e}")
            return "D√©sol√©, une erreur s'est produite lors de la g√©n√©ration de la r√©ponse par l'IA locale."
    else:
        print("\n  -> Le LLM local n'est pas charg√©. Retour du contexte brut.")
        return f"Le LLM local n'est pas disponible. Voici le contexte trouv√©:\n\n{context_string}"


# --- Boucle Principale pour poser des questions (inchang√©e) ---

if __name__ == "__main__":
    print("\n" + "="*60)
    print("üåç SYST√àME DE D√âTECTION DE FAKE NEWS CLIMATIQUES")
    print("Bas√© sur les sources: IPCC, Environmental Defense Fund")
    print("="*60)
    print("\nCommandes disponibles:")
    print("- 'analyze: [affirmation]' : Analyser une affirmation climatique")
    print("- 'question: [question]' : Poser une question g√©n√©rale sur le climat")
    print("- 'quit' ou 'exit' : Quitter")
    print("="*60)

    while True:
        user_input = input("\nVotre entr√©e: ")
        if user_input.lower() in ['quit', 'exit']:
            break
        if not user_input.strip():
            continue

        # D√©terminer le type de requ√™te
        if user_input.lower().startswith('analyze:'):
            claim = user_input[8:].strip()  # Supprimer 'analyze:'
            if claim:
                analysis = analyze_climate_claim(claim)
                print("\n" + "="*50)
                print("üìä ANALYSE DE L'AFFIRMATION")
                print("="*50)
                print(analysis)
                print("="*50)
            else:
                print("Veuillez fournir une affirmation √† analyser apr√®s 'analyze:'")
                
        elif user_input.lower().startswith('question:'):
            question = user_input[9:].strip()  # Supprimer 'question:'
            if question:
                answer = answer_query_rag(question)
                print("\n" + "="*50)
                print("üí¨ R√âPONSE")
                print("="*50)
                print(answer)
                print("="*50)
            else:
                print("Veuillez fournir une question apr√®s 'question:'")
        else:
            # Par d√©faut, traiter comme une analyse de fake news
            analysis = analyze_climate_claim(user_input)
            print("\n" + "="*50)
            print("üìä ANALYSE DE L'AFFIRMATION")
            print("="*50)
            print(analysis)
            print("="*50)

    print("\nüåç Merci d'avoir utilis√© le d√©tecteur de fake news climatiques !")